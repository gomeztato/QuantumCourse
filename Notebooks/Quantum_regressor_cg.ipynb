{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Machine Learning. Regressor\n",
    "\n",
    "Quantum Machine Learning is one the the possible applications of Quantum Computing. This exercise is based on the paper [Mitarai K, Negoro M, Kitagawa M, Fujii K. Quantum circuit learning. Phys Rev A. 2018;98(3):032309.](https://arxiv.org/abs/1803.00745)\n",
    "\n",
    "The example is inspired on [QCL page](http://dkopczyk.quantee.co.uk/qcl/), where a good description of the method is provided. However, to speedup the execution, instead the usual gradient descend algorithm for Machine Learning, a faster optimisation algorithm is used [Conjugate Gradient](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_cg.html)\n",
    "\n",
    "The steps of the algorithm are:\n",
    "\n",
    "1. Starting from state $|0\\rangle$ and an case with input data ${x_i}$ of the dataset ($X$,$Y$), generate an initial Quantun State $|\\Psi({x_i})>$ on the desired number of qubits (which can be larger than the number of input data for a single case). The amplitudes of this Quantum State are the input quantum features of the Machine Learning Model.\n",
    "2. Apply $n$ times a combination of a random Time Evolution of one operator plus a set of parameterised rotations on each qubits.\n",
    "3. Measure the expectation value of one operator of subset of Pauli operators $\\{B_j\\} \\subset {\\{I,X,Y,Z\\}}^{\\otimes N}$. This measurement will be the prediction $y_i$ for this input data ${x_i}$.\n",
    "4. Minimize the cost function which compares the labels $Y$ with the predictions $y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the needed operations from ProjectQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from projectq.cengines import MainEngine\n",
    "from projectq.backends import Simulator\n",
    "from projectq.ops import Z,X,H,Rx,Ry,Rz,All,Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a dataset which follows the funcion $f(x)=x^2$ on the interval $[-1,1]$. This restriction on the input values does not limit the model, because in Classical Machine Learning, usually the input and output data are normalised as a first step.\n",
    "Select the number of cases of the dataset assigning a value to P (take into account that a large number means long simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "P=20\n",
    "data_in=np.linspace(-1,1,P)\n",
    "data_out=data_in*data_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inicialise the state (step 1), apply $R_z(cos^{-1}(x^2))R_y(sin^{-1}(x))$ to all qubits of the Quantum Register. Any other inicialisation is possible. If N i sthe number of qubits, the new state is:\n",
    "\n",
    "$$|\\Psi(x)>= (R_z(cos^{-1}(x^2))R_y(sin^{-1}(x))|0>)^{\\otimes N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Uinit(qreg,x):\n",
    "    import math\n",
    "    theta_1=math.acos(x*x)\n",
    "    theta_2=math.asin(x)\n",
    "    All(Ry(theta_2)) | qreg\n",
    "    All(Rz(theta_1)) | qreg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create now the Unitary operation **U** to train. It receives as input the qubits in a Quantum Register (*qreg*) and an array with the parameters of the rotations which define this unitary operation.\n",
    "\n",
    "For this case, to **each** qubit apply three rotations\n",
    "\n",
    "$$R_x(\\theta_{1j})R_z(\\theta_{2j})R_x(\\theta_{3j})$$ \n",
    "\n",
    "where $j$ is the index of the qubit. So, $\\Theta =\\{\\theta_{ij}\\}$ is an array of paramenters to train of dimensions $[3,N]$, where N is the number of qubits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U(qreg,theta,d):\n",
    "    for index,q in enumerate(qreg):\n",
    "            Rx(theta[0,index,d]) | q\n",
    "            Rz(theta[1,index,d]) | q\n",
    "            Rx(theta[2,index,d]) | q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before this Unitary operation, the state will evolve during a selected time **T** using a Hamiltonian which follows a fully connected transverse Isin model:\n",
    "\n",
    "$$H = \\sum_{j=1}^{N}\\alpha_j X_j + \\sum_{j=1}^{N}\\sum_{k=1}^{j-1}J_{jk}Z_jZ_k$$\n",
    "\n",
    "where $N$ is the number of qubits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from projectq.ops import QubitOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Circuit_Hamiltonian(N,alpha,J):\n",
    "    Hamiltonian=None\n",
    "    k=0\n",
    "    for j in range(N):\n",
    "        if (Hamiltonian is None):\n",
    "            Hamiltonian=alpha[j]*QubitOperator(\"X%d\"%j)\n",
    "        else:\n",
    "            Hamiltonian=Hamiltonian+alpha[j]*QubitOperator(\"X%d\"%j)\n",
    "\n",
    "        for i in range(j):\n",
    "            Hamiltonian=Hamiltonian+J[k]*QubitOperator(\"Z%d Z%d\"%(j,i))\n",
    "            k=k+1\n",
    "    return Hamiltonian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from projectq.ops import TimeEvolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to compose the final circuit. The graphical representation of the circuit is:\n",
    "\n",
    "<img src=\"Images/QCL.jpg\" width=\"60%\"/>\n",
    "\n",
    "The function receives:\n",
    "1. **x** that is the input data for this case\n",
    "2. **Theta** a vector the dimension 3\\*N\\*D+1. Last element multiplies the expectation value before exit the function.\n",
    "3. **N**, the number of qubits \n",
    "4. **D**, the number of times to repeat the evolution\n",
    "5. **T**, the time to evolve th Hamiltonian\n",
    "6. **H**, the Hamiltonian\n",
    "\n",
    "The output must be the expectation value of $\\sigma_z$ for qubit **0** multiplied by the parameter Theta\\[-1\\]\n",
    "\n",
    "Hint: Use the ProjectQ methods:\n",
    "1. [QubitOperator](https://projectq.readthedocs.io/en/latest/projectq.ops.html#projectq.ops.QubitOperator)\n",
    "2. [TimeEvolution](https://projectq.readthedocs.io/en/latest/projectq.ops.html#projectq.ops.TimeEvolution)\n",
    "3. [get_expectation_value](https://projectq.readthedocs.io/en/latest/projectq.backends.html#projectq.backends.Simulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qfun(x,Theta,N,D,T,H):\n",
    "    theta1=Theta[0:3*N*D].reshape(3,N,D)\n",
    "\n",
    "    eng=MainEngine(backend=Simulator(gate_fusion=True, rnd_seed=1000))\n",
    "    qreg=eng.allocate_qureg(N)  \n",
    "    \n",
    "    Uinit(qreg,x)\n",
    "    \n",
    "    for i in range(D):\n",
    "        TimeEvolution(time=T,hamiltonian=H) | qreg\n",
    "        U(qreg,theta1,i)\n",
    "    \n",
    "    eng.flush()\n",
    "    \n",
    "    energy=eng.backend.get_expectation_value(QubitOperator(\"Z0\"),[qreg[0]])\n",
    "    \n",
    "    All(Measure) | qreg\n",
    "    eng.flush()\n",
    "    del eng\n",
    "    \n",
    "    return Theta[-1]*energy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. Define the loss for the training:\n",
    "\n",
    "$$loss = \\frac{1}{m}\\sum_{i=1}^m{(qfun(x_i)-y_i)}^2$$\n",
    "\n",
    "where $m$ is the number of cases and $y_i$ is the label for case $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(Theta,data_in,data_out,N,D,T,H):\n",
    "    fun_out=0\n",
    "    for i in range(len(data_in)):\n",
    "        x=data_in[i]\n",
    "        y=data_out[i]\n",
    "        y_pred=qfun(x,Theta,N,D,T,H)\n",
    "        fun_out+=(y_pred-y)**2\n",
    "        \n",
    "    return fun_out/len(data_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(Theta,data_in,data_out,N,D,T,H):\n",
    "    import random\n",
    "    import math\n",
    "    fun_out=np.zeros(len(data_in))\n",
    "    for i in range(len(data_in)):\n",
    "        x=data_in[i]\n",
    "        y=data_out[i]\n",
    "        y_pred=qfun(x,Theta,N,D,T,H)\n",
    "        fun_out[i]=(y_pred-y)\n",
    "    grad=np.zeros(len(Theta))\n",
    "    for j in range(len(Theta)-1):\n",
    "        grad[j]=0.\n",
    "        for i in range(len(data_in)):\n",
    "            x=data_in[i]\n",
    "            Theta_in=Theta.copy()\n",
    "            Theta_in[j]=Theta[j]+(math.pi*0.5)\n",
    "            bplus=qfun(x,Theta_in,N,D,T,H)\n",
    "            Theta_in[j]=Theta[j]-(math.pi*0.5)\n",
    "            bminus=qfun(x,Theta_in,N,D,T,H)\n",
    "            grad[j]+=(bplus-bminus)*fun_out[i]\n",
    "    \"\"\"\n",
    "    The last weight is special, because it is not a rotation.\n",
    "    \"\"\"\n",
    "    grad[-1]=0.\n",
    "    for i in range(len(data_in)):\n",
    "        x=data_in[i]\n",
    "        grad[-1]+=2.*fun_out[i]*qfun(x,Theta,N,D,T,H)\n",
    "    grad[-1]=grad[-1]/Theta[-1]\n",
    "    grad=grad/len(data_in)\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the training parameters. The angles of the rotations must be selected randomly in the interval $[0,2\\pi]$, meanwhile the factor of the expectation value is initialised to 1.0\n",
    "\n",
    "Select also the number of repetitions (D) and the time for the Hamiltonian evolution (T)\n",
    "\n",
    "**Hint**: if you want to speedup the convergence, use these pre-trained values for N=3, D=2 and T=10.\n",
    "\n",
    "alpha= np.array([-0.09437294,  0.90506988, -0.67730307 ])\n",
    "\n",
    "J= np.array([-0.62281193, -0.59477404, -0.24760364])\n",
    "\n",
    "Theta= np.array([ 1.67414239, 5.81058199, 5.48748225,  4.76538875,  0.34889076,  5.14003758,\n",
    "  6.46545405,  4.59147751,  4.80588215,  4.51748371,  3.73661614,  5.55503066,\n",
    "  2.32459032,  1.04557867,  4.48241849,  5.51983066, -0.33860855,  5.99787863,\n",
    "  2.00313909])\n",
    "\n",
    " and select the number of iterations to 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=3\n",
    "D=2\n",
    "T=10\n",
    "alpha=(np.random.rand(N)-0.5)*2.0\n",
    "J=(np.random.rand(N*(N-1)//2)-0.5)*2.0\n",
    "Theta=np.random.rand(3*N*D+1)\n",
    "Theta[0:3*N*D]=np.pi*2.*Theta[0:3*N*D]\n",
    "Theta[-1]=1.0\n",
    "\"\"\"\n",
    "UNCOMMENT TO SPEEP-UP\n",
    "\n",
    "alpha= np.array([ -0.09437294,  0.90506988, -0.67730307 ])\n",
    "J= np.array([-0.62281193,-0.59477404, -0.24760364])\n",
    "Theta= np.array([ 1.67414239, 5.81058199, 5.48748225,  4.76538875,  0.34889076,  5.14003758,\n",
    "  6.46545405,  4.59147751,  4.80588215,  4.51748371,  3.73661614,  5.55503066,\n",
    "  2.32459032,  1.04557867,  4.48241849,  5.51983066, -0.33860855,  5.99787863,\n",
    "  2.00313909])\n",
    "\"\"\"\n",
    "Hamiltonian=Circuit_Hamiltonian(N,alpha,J)\n",
    "print(\"The value of the initial loss is =\",loss(Theta,data_in,data_out,N,D,T,Hamiltonian))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to check that the Jacobian is correct, comparing the result against the numerical gradient for the first point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad=jacobian(Theta,data_in[0:1],data_out[0:1],N,D,T,Hamiltonian)\n",
    "h=0.000001\n",
    "for i in range(len(Theta)):\n",
    "    Theta_in=Theta.copy()\n",
    "    Theta_in[i]=Theta[i]+h\n",
    "    h1=loss(Theta_in,data_in[0:1],data_out[0:1],N,D,T,Hamiltonian)\n",
    "    Theta_in[i]=Theta[i]-h\n",
    "    h2=loss(Theta_in,data_in[0:1],data_out[0:1],N,D,T,Hamiltonian)\n",
    "    grad2=(h1-h2)/(2*h)\n",
    "    print(\"Grad[%d]:%.5f - NumericalGrad[%d]:%.5f\"%(i,grad[i],i,grad2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the initial form of the proposed function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "out=[qfun(j,Theta,N,D,T,Hamiltonian) for j in data_in]\n",
    "plt.plot(data_in,out,label=\"Init model\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.plot(data_in,data_out,\"*\",label=\"Labels\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a callback function to show how the training evolves. **Just execute the next cell without changes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(xk):\n",
    "    cost=loss(xk,data_in,data_out,N,D,T,Hamiltonian)\n",
    "    out=[qfun(j,xk,N,D,T,Hamiltonian) for j in data_in]\n",
    "    plt.plot(data_in,out,\"b\",label=\"Cost=%.4f\"%cost)\n",
    "    plt.plot(data_in,data_in*data_in,\"*\",label=\"Labels\")\n",
    "    out=[qfun(j,Theta,N,D,T,Hamiltonian) for j in data_in]\n",
    "    plt.plot(data_in,out,\"r\",label=\"First\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"Cost=\",cost)\n",
    "    print(\"Theta=\",xk)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Now train the model. Select the number of iterations in maxiter\n",
    "\n",
    "Use the minimize method with \"Conjugate Gradient (CG)\" as algorithm of minimisation. Do not forget to add the callback, so you can follow the training evolution.\n",
    "\n",
    "**Hint**: [Minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import math\n",
    "options={'maxiter': 40, 'disp': True}\n",
    "args=(data_in,data_out,N,D,T,Hamiltonian)\n",
    "Theta_out=minimize(loss,Theta,args=args, jac=jacobian,callback=callback,method='CG', options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#qfun(j,Theta,N,D,T)\n",
    "out=[qfun(j,Theta_out.x,N,D,T,Hamiltonian) for j in data_in]\n",
    "plt.plot(data_in,out,\"b\",label=\"QML model\")\n",
    "plt.plot(data_in,data_in*data_in,\"*\", label=\"Labels\")\n",
    "out=[qfun(j,Theta,N,D,T,Hamiltonian) for j in data_in]\n",
    "plt.plot(data_in,out,\"r\",label=\"Initial model\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"X\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to store all the parameters that define this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"alpha=\",alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"J=\",J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Theta=\",Theta_out.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
